Here is the **complete, full-stack project**.

It features a flexible **Multi-Backend Architecture** that switches between the official Google Gemini API (using the specific model you requested) and any OpenAI-compatible endpoint (for Local LLMs, vLLM, or OpenAI).

It uses the "weird" default port **43210**.

### üìÇ Project Structure

```text
grounding-mcp/
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ grounding_server.py
‚îú‚îÄ‚îÄ test_grounding.py
‚îú‚îÄ‚îÄ Dockerfile
‚îî‚îÄ‚îÄ README.md
```

---

### 1. `requirements.txt`

Includes both Google and OpenAI SDKs to support all modes.

```text
fastmcp==0.3.2
google-genai>=0.3.0
openai>=1.30.0
pillow>=10.0.0
tenacity>=8.2.0
pydantic>=2.0.0
pytest
```

---

### 2. `grounding_server.py` (The Core Logic)

This file contains the logic to switch backends and the "weird" port configuration.

```python
import os
import base64
import json
import logging
import hashlib
from io import BytesIO
from typing import Dict, List, Optional, Any

from fastmcp import FastMCP
from PIL import Image, ExifTags
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

# --- Backend SDKs ---
# We import these conditionally or handle errors if config is missing, 
# but for this script we assume requirements are installed.
try:
    from google import genai
    from google.genai import types as google_types
except ImportError:
    genai = None

try:
    from openai import OpenAI, APIConnectionError, RateLimitError
except ImportError:
    OpenAI = None

# --- Configuration & Logging ---
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("grounding_server")

# üîß CONFIGURATION
WEIRD_PORT = int(os.environ.get("PORT", 43210))  # Default Weird Port
PROVIDER = os.environ.get("GROUNDING_PROVIDER", "google").lower() # 'google' or 'openai'
API_KEY = os.environ.get("API_KEY")
BASE_URL = os.environ.get("BASE_URL") # Optional, for local LLMs
MODEL_NAME = os.environ.get("MODEL_NAME") # Overrides default if set

# Defaults
DEFAULT_GOOGLE_MODEL = "models/gemini-flash-latest"
MAX_IMAGE_BYTES = 6 * 1024 * 1024
MAX_DIMENSION = 8000
ALLOWED_TYPES = {"button", "input", "icon", "text", "link", "image", "other"}

# Initialize FastMCP
mcp = FastMCP("Blind Agent Visual Cortex", dependencies=["google-genai", "openai", "pillow"])

# --- JSON Schema (Backend Agnostic) ---
RESPONSE_SCHEMA_DICT = {
    "type": "object",
    "properties": {
        "summary": {"type": "string"},
        "components": {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "id": {"type": "integer"},
                    "label": {"type": "string"},
                    "type": {"type": "string"},
                    "tags": {"type": "array", "items": {"type": "string"}},
                    "box_2d": {
                        "type": "array",
                        "items": {"type": "number"},
                        "description": "[ymin, xmin, ymax, xmax] normalized 0-1000"
                    }
                },
                "required": ["id", "label", "type", "box_2d"]
            }
        }
    },
    "required": ["summary", "components"]
}

# --- Image Helpers ---

def _fix_orientation(img: Image.Image) -> Image.Image:
    try:
        exif = img._getexif()
        if not exif: return img
        orientation = exif.get(274) # 274 is the Orientation tag ID
        if orientation == 3: return img.rotate(180, expand=True)
        elif orientation == 6: return img.rotate(270, expand=True)
        elif orientation == 8: return img.rotate(90, expand=True)
    except Exception:
        pass
    return img

def scale_box(box_norm: List[float], width: int, height: int) -> Dict[str, int]:
    """Converts 0-1000 normalized coords to pixels."""
    if not box_norm or len(box_norm) != 4:
        return {"error": "invalid_box"}
    
    y1, x1, y2, x2 = [max(0.0, min(1000.0, float(v))) for v in box_norm]

    abs_y1 = int(round((y1 / 1000.0) * height))
    abs_x1 = int(round((x1 / 1000.0) * width))
    abs_y2 = int(round((y2 / 1000.0) * height))
    abs_x2 = int(round((x2 / 1000.0) * width))

    # Fix degenerate
    if abs_x2 <= abs_x1: abs_x2 = min(width, abs_x1 + 1)
    if abs_y2 <= abs_y1: abs_y2 = min(height, abs_y1 + 1)

    return {
        "center_x": (abs_x1 + abs_x2) // 2,
        "center_y": (abs_y1 + abs_y2) // 2,
        "width": abs_x2 - abs_x1,
        "height": abs_y2 - abs_y1,
        "y_min": abs_y1, "x_min": abs_x1, "y_max": abs_y2, "x_max": abs_x2
    }

# --- Backend Implementation ---

class BackendHandler:
    def generate(self, image_bytes: bytes, prompt: str) -> Dict:
        raise NotImplementedError

class GoogleBackend(BackendHandler):
    def __init__(self, key: str, model: str):
        if not genai: raise ImportError("google-genai package not installed")
        self.client = genai.Client(api_key=key)
        # Requirement: "THE MODEl to use is models/gemini-flash-latest"
        self.model = model if model else DEFAULT_GOOGLE_MODEL
        logger.info(f"Initialized Google Backend with model: {self.model}")

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def generate(self, image_bytes: bytes, prompt: str) -> Dict:
        response = self.client.models.generate_content(
            model=self.model,
            contents=[
                google_types.Part.from_bytes(data=image_bytes, mime_type="image/png"),
                prompt
            ],
            config=google_types.GenerateContentConfig(
                response_mime_type="application/json",
                response_schema=RESPONSE_SCHEMA_DICT,
                temperature=0.0
            ),
            http_options=google_types.HttpOptions(timeout=45.0)
        )
        
        # Handle different SDK versions parsing
        if hasattr(response, 'parsed') and response.parsed:
            return response.parsed if isinstance(response.parsed, dict) else response.parsed.model_dump()
        return json.loads(response.text)

class OpenAIBackend(BackendHandler):
    def __init__(self, key: str, base_url: str, model: str):
        if not OpenAI: raise ImportError("openai package not installed")
        self.client = OpenAI(api_key=key, base_url=base_url)
        # Fallback to gpt-4o-mini if no model provided for OpenAI mode
        self.model = model if model else "gpt-4o-mini"
        logger.info(f"Initialized OpenAI/Local Backend with model: {self.model} at {base_url or 'default url'}")

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def generate(self, image_bytes: bytes, prompt: str) -> Dict:
        b64_img = base64.b64encode(image_bytes).decode('utf-8')
        
        # Append schema instruction to prompt for generic models
        json_instruction = f"""
        You must return valid JSON. 
        Schema: {json.dumps(RESPONSE_SCHEMA_DICT)}
        Ensure all bounding boxes 'box_2d' are [ymin, xmin, ymax, xmax] normalized to 0-1000.
        """
        
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt + "\n" + json_instruction},
                    {
                        "type": "image_url", 
                        "image_url": {"url": f"data:image/png;base64,{b64_img}"}
                    }
                ]
            }
        ]

        response = self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            response_format={"type": "json_object"},
            temperature=0.0,
            max_tokens=2048
        )
        return json.loads(response.choices[0].message.content)

# --- Initialization Logic ---

if not API_KEY:
    logger.warning("API_KEY is not set. Requests may fail.")

handler = None
if PROVIDER == "google":
    handler = GoogleBackend(API_KEY, MODEL_NAME)
else:
    handler = OpenAIBackend(API_KEY, BASE_URL, MODEL_NAME)

# --- The Tool ---

@mcp.tool()
def analyze_screenshot(image_base64: str) -> str:
    """
    Grounding Tool: Takes a Base64 screenshot, returns JSON with pixel coordinates (box_px) 
    for UI elements (buttons, inputs, etc).
    """
    try:
        # 1. Image Loading
        if not image_base64: return json.dumps({"error": "empty_input"})
        image_bytes = base64.b64decode(image_base64)
        
        if len(image_bytes) > MAX_IMAGE_BYTES:
            return json.dumps({"error": "image_too_large_max_6mb"})
            
        img = Image.open(BytesIO(image_bytes))
        img = _fix_orientation(img)
        width, height = img.size
        
        if width > MAX_DIMENSION or height > MAX_DIMENSION:
            return json.dumps({"error": "dimensions_too_large"})

        # 2. Prompting
        prompt = """
        Analyze this UI screenshot for a blind agent.
        1. Identify the screen context.
        2. Detect all interactive elements.
        3. Return JSON with 'box_2d' [ymin, xmin, ymax, xmax] in 0-1000 coordinates.
        """

        # 3. Backend Call
        try:
            data = handler.generate(image_bytes, prompt)
        except Exception as e:
            logger.error(f"Backend failed: {e}")
            return json.dumps({"error": "ai_provider_error", "details": str(e)})

        # 4. Processing & Scaling
        final_components = []
        raw_comps = data.get("components", [])
        
        for i, c in enumerate(raw_comps):
            try:
                # Basic cleanup
                c_id = c.get("id", i+1)
                c_label = c.get("label", "unknown")
                c_type = c.get("type", "other")
                if c_type not in ALLOWED_TYPES: c_type = "other"
                
                # Scale
                box = c.get("box_2d", [0,0,0,0])
                box_px = scale_box(box, width, height)
                
                final_components.append({
                    "id": c_id,
                    "label": c_label,
                    "type": c_type,
                    "tags": c.get("tags", []),
                    "box_norm": box,
                    "box_px": box_px
                })
            except Exception:
                continue # Skip malformed items

        result = {
            "summary": data.get("summary", ""),
            "resolution": {"width": width, "height": height},
            "components": final_components
        }
        
        return json.dumps(result, indent=2)

    except Exception as e:
        logger.exception("Critical error")
        return json.dumps({"error": "internal_error", "details": str(e)})

if __name__ == "__main__":
    # üöÄ WEIRD PORT ACTIVATED
    logger.info(f"Starting Grounding MCP Server on port {WEIRD_PORT}...")
    mcp.run(port=WEIRD_PORT)
```

---

### 3. `.env.example`

Configurable for any provider.

```bash
# === OPTION 1: GOOGLE (Recommended) ===
GROUNDING_PROVIDER=google
# The key for Gemini
API_KEY=your_google_ai_studio_key
# Defaults to "models/gemini-flash-latest" if left blank
MODEL_NAME=

# === OPTION 2: LOCAL / OPENAI / vLLM ===
# GROUNDING_PROVIDER=openai
# API_KEY=sk-proj-... or "not-needed" for local
# BASE_URL=http://localhost:1234/v1
# MODEL_NAME=llama-3.2-vision-instruct

# === SERVER CONFIG ===
# The Weird Port
PORT=43210
```

---

### 4. `Dockerfile`

Optimized image using the weird port exposure.

```dockerfile
FROM python:3.11-slim

ENV PYTHONUNBUFFERED=1
WORKDIR /app

# Install system libs for Pillow
RUN apt-get update && apt-get install -y libjpeg-dev zlib1g-dev && rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY grounding_server.py .

# Expose the weird port
EXPOSE 43210

# Default command
CMD ["python", "grounding_server.py"]
```

---

### 5. `test_grounding.py`

Verifies that the architecture loads the correct backend logic.

```python
import os
from unittest.mock import MagicMock
import pytest
from grounding_server import scale_box, GoogleBackend, OpenAIBackend

def test_scale_logic():
    # 1000x1000 screen, box is [0,0, 500,500] (top left quadrant)
    res = scale_box([0, 0, 500, 500], 1000, 1000)
    assert res['center_x'] == 250
    assert res['center_y'] == 250

def test_google_init():
    # Test that Google backend initializes with the required model
    with pytest.raises(Exception): 
        # Should fail without API key / mocks
        GoogleBackend("fake-key", "")
        
def test_openai_init():
    backend = OpenAIBackend("fake", "http://local", "custom-model")
    assert backend.model == "custom-model"
```

---

### 6. `README.md`

Explains the setup clearly.

```markdown
# üëÅÔ∏è Visual Cortex MCP Server

A specialized MCP server that provides "vision" to blind AI agents. It detects UI elements and returns **pixel-perfect coordinates** for automation.

It runs on the **Weird Port 43210** by default.

## üöÄ Supported Backends

### 1. Google Gemini (Default & Best)
*   **Model:** `models/gemini-flash-latest`
*   **Performance:** Fastest, native 2D grounding support.
*   **Cost:** Extremely low.

### 2. OpenAI / Local Compatible
*   **Model:** Configurable (User defined).
*   **Support:** Works with `gpt-4o`, `vLLM` hosting Llama 3.2 Vision, or `LM Studio`.

## üõ†Ô∏è Installation

1.  **Clone & Install:**
    ```bash
    pip install -r requirements.txt
    ```

2.  **Configure Environment:**
    Copy `.env.example` to `.env` and set your provider.

    *For Google:*
    ```bash
    export GROUNDING_PROVIDER=google
    export API_KEY=your_gemini_key
    ```

    *For Local LLM:*
    ```bash
    export GROUNDING_PROVIDER=openai
    export BASE_URL=http://localhost:1234/v1
    export MODEL_NAME=llama-3.2-vision
    export API_KEY=local
    ```

3.  **Run:**
    ```bash
    python grounding_server.py
    ```
    *Server listening on port 43210*

## ü§ñ Usage

**Input:** Base64 Encoded Image.
**Output:** JSON with `box_px` (Absolute pixel coordinates).

### Agent Instruction
> "Use `analyze_screenshot` to see. The tool returns a list of components. Use the `center_x` and `center_y` inside `box_px` to click elements."

## üê≥ Docker

```bash
docker build -t visual-cortex .
docker run -p 43210:43210 --env-file .env visual-cortex
```
```
